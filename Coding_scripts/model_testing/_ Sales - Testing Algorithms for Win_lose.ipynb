{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d60aa-d9bd-4201-afda-80d77f1734a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Function to make sure the script autostops, in case of wrong lakehouse destination\n",
    "\n",
    "def lakehouse_gatekeep(expected_db = \"ml_curate_lakehouse\"):\n",
    "    db = spark.catalog.currentDatabase()\n",
    "\n",
    "    if db != expected_db:\n",
    "       raise Exception(\n",
    "        f\"Forkert Lakehouse er tilkoblet som default: '{db}'.\"\n",
    "        f\"Det rigtige lakehouse er: '{expected_db}'. Sæt default lakehouse til '{expected_db}' i Explorer i venstre side.\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Det korrekte lakehouse: '{db}' er valgt. scriptet fortsættes\")\n",
    "\n",
    "lakehouse_gatekeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09269c7d-c410-4f67-bbc5-c9e830148e0e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#### This script will be used to test different algorithms for the Win/lose model thats working for the active pipeline #############################\n",
    "#### It is intended to test different algorithms to both ensure and validate that the usage of XGB over other algorithms was the correct choice. ####\n",
    "\n",
    "# The following scripts will be tested within the scripts\n",
    "    # - XGB\n",
    "    # - LightGBM\n",
    "    # - CatBoost\n",
    "\n",
    "# I've chosen to use decisions trees, more specifically decision trees that are very good at handling the data structure thats present within the data;\n",
    "# a mixed structure of both catvars and numvars\n",
    "\n",
    "# Some more known decisiontrees such as AdaBoost and RandomForest has been excluded from the getgo due to their harsh handling of categorical variables and the need of one hot encoding. \n",
    "# One hot encoding would simply explode the dataset due to the sheer amount of levels the categories has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ff7ee-cfd5-47b1-8e66-67d5a671bbab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6ebb3-e988-4a9c-9dd8-07e59d85d306",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f729d-13ae-4e5e-8c8c-488922da05ed",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ac645-0df0-42a2-89ba-798fbc39e4a1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#### Data retrieval and preparation ####\n",
    "\n",
    "\n",
    "solutions_ml = spark.read.format(\"delta\").load(\"cantshow\")\n",
    "solutions_ml = solutions_ml.toPandas()\n",
    "\n",
    "# Lakehouse import/export automatically changed categorical variable to object types, therefore: \n",
    "\n",
    "\n",
    "for col in solutions_ml.select_dtypes(include = 'object').columns:\n",
    "    solutions_ml[col] = solutions_ml[col].astype('category')\n",
    "\n",
    "solutions_ml['Is_EOQ'] = solutions_ml['Is_EOQ'].astype('category') \n",
    "solutions_ml['Quarter'] = solutions_ml['Quarter'].astype('category') \n",
    "solutions_ml['YearName'] = solutions_ml['YearName'].astype('category') \n",
    "\n",
    "\n",
    "X = solutions_ml.drop(columns=['OpportunityState',\n",
    "'OpportunityStatusGroup',\n",
    "'OpportunityNumber',\n",
    "'CloseDate',\n",
    "'IndustryName',\n",
    "'OpportunitySalesTeam',\n",
    "'AnnualContractValueDKK',\n",
    "'MarginValueDKK',\n",
    "'YearName'\n",
    "#'Title',\n",
    "#'Site', \n",
    "#'Department'\n",
    "#'OpportunityState_num'\n",
    "])\n",
    "\n",
    "y = solutions_ml[['OpportunityState']]\n",
    "\n",
    "opp_id = solutions_ml[['OpportunityNumber']]\n",
    "\n",
    "\n",
    "## Mapper won: 1 & lost: 0\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "y = pd.Series(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y,\n",
    "    test_size = 0.3,\n",
    "    stratify = y,\n",
    "    random_state = 123\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb2213-533e-4e5e-95cd-9f59ada8473d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea42db5-6a16-4fb8-805b-71ed341e650a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# showcasing performance of algorithms Prior to Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81666a3-cc7d-4f08-a333-220912c980d8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "#### CatBoost ####\n",
    "##################\n",
    "\n",
    "#### CatBoost ####\n",
    "\n",
    "cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Create CatBoostClassifier\n",
    "sales_cb = CatBoostClassifier(\n",
    "    iterations = 100,\n",
    "    learning_rate = 1,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "sales_cb.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    cat_features=cat_features,\n",
    "    eval_set=(X_test, y_test)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadf4d8-9aba-4fb1-acc7-b55e3422a0a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#### Validating the CatBoost Model ####\n",
    "\n",
    "# Predict probabilities\n",
    "y_train_proba = sales_cb.predict_proba(X_train)[:, 1]\n",
    "y_test_proba  = sales_cb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.5\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_test_pred  = (y_test_proba  >= threshold).astype(int)\n",
    "\n",
    "# Classification reports\n",
    "print(\"Træningsdata:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Testdata:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Importance (CatBoost)\n",
    "# ------------------------------\n",
    "\n",
    "# CatBoost feature importance is accessed via get_feature_importance()\n",
    "model_featureimportance = sales_cb.get_feature_importance()\n",
    "\n",
    "grid_fe = pd.DataFrame({\n",
    "    \"Importance\": model_featureimportance,\n",
    "    \"Feature\": X_train.columns\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(grid_fe)\n",
    "\n",
    "# ------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ------------------------------\n",
    "\n",
    "perm_imp_grid = permutation_importance(\n",
    "    sales_cb,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "perm_imp_grid_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': perm_imp_grid.importances_mean,\n",
    "    'Std': perm_imp_grid.importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(\n",
    "    perm_imp_grid_df['Feature'],\n",
    "    perm_imp_grid_df['Importance'],\n",
    "    xerr=perm_imp_grid_df['Std']\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Mean Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02da61-cc3b-4214-b849-58b9a3173ca0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "#### LightGBM ####\n",
    "##################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sales_lgb = lgb.LGBMClassifier()\n",
    "\n",
    "\n",
    "sales_lgb.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='auc',\n",
    "    categorical_feature= cat_features\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5291943-ad1d-4e84-ae2a-bdd3eb0e79c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#### LightGBM ####\n",
    "\n",
    "\n",
    "#### Validating the LightGBM Model ####\n",
    "\n",
    "# Predict probabilities\n",
    "y_train_proba = sales_lgb.predict_proba(X_train)[:, 1]\n",
    "y_test_proba  = sales_lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.5\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_test_pred  = (y_test_proba  >= threshold).astype(int)\n",
    "\n",
    "# Classification reports\n",
    "print(\"Træningsdata:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Testdata:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Importance (CatBoost)\n",
    "# ------------------------------\n",
    "\n",
    "model_featureimportance = sales_lgb.feature_importances_\n",
    "\n",
    "grid_fe = pd.DataFrame({\n",
    "    \"Importance\": model_featureimportance,\n",
    "    \"Feature\": X_train.columns\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(grid_fe)\n",
    "\n",
    "# ------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ------------------------------\n",
    "\n",
    "perm_imp_grid = permutation_importance(\n",
    "    sales_lgb,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "perm_imp_grid_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': perm_imp_grid.importances_mean,\n",
    "    'Std': perm_imp_grid.importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(\n",
    "    perm_imp_grid_df['Feature'],\n",
    "    perm_imp_grid_df['Importance'],\n",
    "    xerr=perm_imp_grid_df['Std']\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Mean Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ec6ad-e211-4e96-b663-b232edf6386a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#### XGBoost ####\n",
    "#################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "sales_xgb = XGBClassifier(\n",
    "    iterations = 10,\n",
    "    learning_rate = 1,\n",
    "    enable_categorical = True\n",
    ")\n",
    "\n",
    "sales_xgb.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac7995-af28-45e2-a97e-09567d46abaf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#### Validating the CatBoost Model ####\n",
    "\n",
    "# Predict probabilities\n",
    "y_train_proba = sales_xgb.predict_proba(X_train)[:, 1]\n",
    "y_test_proba  = sales_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.5\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_test_pred  = (y_test_proba  >= threshold).astype(int)\n",
    "\n",
    "# Classification reports\n",
    "print(\"Træningsdata:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Testdata:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Importance (CatBoost)\n",
    "# ------------------------------\n",
    "\n",
    "model_featureimportance = sales_xgb.get_feature_importance()\n",
    "\n",
    "grid_fe = pd.DataFrame({\n",
    "    \"Importance\": model_featureimportance,\n",
    "    \"Feature\": X_train.columns\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(grid_fe)\n",
    "\n",
    "# ------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ------------------------------\n",
    "\n",
    "perm_imp_grid = permutation_importance(\n",
    "    sales_xgb,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "perm_imp_grid_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': perm_imp_grid.importances_mean,\n",
    "    'Std': perm_imp_grid.importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(\n",
    "    perm_imp_grid_df['Feature'],\n",
    "    perm_imp_grid_df['Importance'],\n",
    "    xerr=perm_imp_grid_df['Std']\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Mean Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e53db-e525-42bc-b885-706221fde614",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Testing the following algorithm/model: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a013bb-7f47-4aea-922b-8b6aa18e0582",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# categorical feature indices\n",
    "cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "cb_model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"Accuracy\", \n",
    "    verbose=False,\n",
    "    random_seed=1234\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    \"iterations\": [300, 500, 700, 900],\n",
    "    \"learning_rate\": np.linspace(0.01, 0.2, 10),\n",
    "    \"depth\": [3, 4, 5, 6],\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 7, 10],\n",
    "    \"rsm\": [0.5, 0.7, 0.85, 1.0],\n",
    "    \"bagging_temperature\": [0, 0.5, 1, 2],\n",
    "    \"random_strength\": [1, 2, 3, 5]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train, cat_features=cat_features)\n",
    "\n",
    "print(\"Best params:\", random_search.best_params_)\n",
    "print(\"Best balanced CV accuracy:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea810012-69b1-43f6-8df8-fc1e2757e356",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#### CatBoost ####\n",
    "\n",
    "cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Create CatBoostClassifier\n",
    "sales_cb = CatBoostClassifier(\n",
    "    iterations=700,         \n",
    "    learning_rate=0.157,\n",
    "    depth=4,                 \n",
    "    l2_leaf_reg = 1,           \n",
    "    random_seed=1234,\n",
    "    loss_function=\"Logloss\", \n",
    "    eval_metric=\"Accuracy\",\n",
    "    verbose=False,      \n",
    "    rsm = 0.5,\n",
    "    bagging_temperature = 0.5,\n",
    "    random_strength = 3,\n",
    "    auto_class_weights='Balanced'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sales_cb.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    cat_features=cat_features,\n",
    "    eval_set=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa757d2-b037-40ec-98a1-02a136552cfb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#### Validating the CatBoost Model ####\n",
    "\n",
    "# Predict probabilities\n",
    "y_train_proba = sales_cb.predict_proba(X_train)[:, 1]\n",
    "y_test_proba  = sales_cb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.5\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_test_pred  = (y_test_proba  >= threshold).astype(int)\n",
    "\n",
    "# Classification reports\n",
    "print(\"Træningsdata:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Testdata:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Importance (CatBoost)\n",
    "# ------------------------------\n",
    "\n",
    "model_featureimportance = sales_cb.get_feature_importance()\n",
    "\n",
    "grid_fe = pd.DataFrame({\n",
    "    \"Importance\": model_featureimportance,\n",
    "    \"Feature\": X_train.columns\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(grid_fe)\n",
    "\n",
    "# ------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ------------------------------\n",
    "\n",
    "perm_imp_grid = permutation_importance(\n",
    "    sales_cb,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "perm_imp_grid_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': perm_imp_grid.importances_mean,\n",
    "    'Std': perm_imp_grid.importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(\n",
    "    perm_imp_grid_df['Feature'],\n",
    "    perm_imp_grid_df['Importance'],\n",
    "    xerr=perm_imp_grid_df['Std']\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Mean Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa44e91-ff14-4a00-84d3-3e85e96526e4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)\n",
    "\n",
    "thresholds = np.arange(0.1, 0.91, 0.05)\n",
    "threshold_scores = {t: [] for t in thresholds}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    sales_cb.fit(X_train, y_train,\n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "    y_val_proba = sales_cb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Evaluate each threshold\n",
    "    for t in thresholds:\n",
    "        y_val_pred = (y_val_proba >= t).astype(int)\n",
    "        score = balanced_accuracy_score(y_val, y_val_pred)\n",
    "        threshold_scores[t].append(score)\n",
    "\n",
    "    print(f\"Fold {fold} complete.\")\n",
    "\n",
    "# Compute mean accuracy per threshold\n",
    "mean_scores = {t: np.mean(scores) for t, scores in threshold_scores.items()}\n",
    "best_threshold = max(mean_scores, key=mean_scores.get)\n",
    "best_score = mean_scores[best_threshold]\n",
    "\n",
    "print(\"\\n Threshold Tuning Results:\")\n",
    "for t in sorted(mean_scores):\n",
    "    print(f\"Threshold {t:.2f}: Mean Balanced Accuracy = {mean_scores[t]:.4f}\")\n",
    "\n",
    "print(f\"\\n Best Threshold: {best_threshold:.2f} with Mean Balanced Accuracy = {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc29ebe-35ff-4d9e-9c1a-0b386d9ae2cb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "threshold = 0.5\n",
    "\n",
    "val_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train_fold = X.iloc[train_idx].copy()\n",
    "    X_val_fold   = X.iloc[val_idx].copy()\n",
    "    y_train_fold = y.iloc[train_idx].copy()\n",
    "    y_val_fold   = y.iloc[val_idx].copy()\n",
    "\n",
    "    sales_cb.fit(X_train_fold, y_train_fold, cat_features=cat_features)\n",
    "\n",
    "    y_train_proba = sales_cb.predict_proba(X_train_fold)[:, 1]\n",
    "    y_val_proba   = sales_cb.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "    y_val_pred   = (y_val_proba >= threshold).astype(int)\n",
    "\n",
    "    train_score = balanced_accuracy_score(y_train_fold, y_train_pred)\n",
    "    val_score   = balanced_accuracy_score(y_val_fold, y_val_pred)\n",
    "\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"    Training Balanced Accuracy:   {train_score:.4f}\")\n",
    "    print(f\"    Validation Balanced Accuracy: {val_score:.4f}\\n\")\n",
    "\n",
    "# Show mean CV score\n",
    "print(f\"Mean Validation Balanced Accuracy: {np.mean(val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48914b8-ce9f-4f05-ab90-0b647e585af0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Testing the following algorithm/model: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc62b1e-bb9a-4c19-b9e9-af0ef2254afa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#### LightGBM RandomizedSearch to check different hypertuning combinations ####\n",
    "\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "for c in cat_cols:\n",
    "    X_train[c] = X_train[c].astype(\"category\")\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    boosting_type='gbdt',\n",
    "    random_state=1234,\n",
    "    n_jobs=-1,\n",
    "    class_weight = \"balanced\"\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [300, 500, 700, 900],           # equivalent to CatBoost 'iterations'\n",
    "    \"learning_rate\": np.linspace(0.01, 0.2, 10),\n",
    "    \"max_depth\": [3, 4, 5, 6],                      # equivalent to CatBoost 'depth'\n",
    "    \"reg_alpha\": [0, 1, 3, 5, 7],                   # equivalent-ish to 'l2_leaf_reg'\n",
    "    \"reg_lambda\": [0, 1, 3, 5, 7],                  # add regularization flexibility\n",
    "    \"subsample\": [0.7, 0.85, 1.0],                  # similar to 'rsm/bagging_temperature'\n",
    "    \"colsample_bytree\": [0.5, 0.7, 0.85, 1.0],\n",
    "    \"min_child_samples\": [10, 20, 50], \n",
    "    \"num_leaves\": [15, 31, 63, 127]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,                    # Number of random combinations\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    random_state=1234\n",
    ")\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", random_search.best_params_)\n",
    "print(\"Best CV score:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83ade6-0b93-48ba-be66-8ccaa61a7408",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "for c in cat_cols:\n",
    "    X_train[c] = X_train[c].astype('category')\n",
    "    X_test[c] = X_test[c].astype('category')\n",
    "\n",
    "sales_lgb = lgb.LGBMClassifier(\n",
    "    n_estimators=500,        # from your best params\n",
    "    learning_rate = 0.1788,\n",
    "    max_depth=6,\n",
    "    reg_lambda = 7,\n",
    "    reg_alpha =7,        # roughly corresponds to CatBoost l2_leaf_reg\n",
    "    subsample=0.7,           # adjust according to your best params (rsm/bagging equivalent)\n",
    "    colsample_bytree=0.85,    # corresponds to rsm\n",
    "    min_child_samples=50, \n",
    "    num_leaves = 63,\n",
    "    random_state=1234,\n",
    "    objective='binary',\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=-1,\n",
    "    class_weight = \"balanced\"\n",
    ")\n",
    "\n",
    "sales_lgb.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='auc',   \n",
    "    categorical_feature=cat_cols\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e242e9-2d04-4384-9f0c-ccb9c4315ca5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#### LightGBM ####\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#### Validating the LGBM Model ####\n",
    "\n",
    "y_train_proba = sales_lgb.predict_proba(X_train)[:, 1]\n",
    "y_test_proba  = sales_lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_test_pred  = (y_test_proba  >= threshold).astype(int)\n",
    "\n",
    "# Classification reports\n",
    "print(\"Træningsdata:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Testdata:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Importance (CatBoost)\n",
    "# ------------------------------\n",
    "\n",
    "model_featureimportance = sales_lgb.feature_importances_\n",
    "\n",
    "grid_fe = pd.DataFrame({\n",
    "    \"Importance\": model_featureimportance,\n",
    "    \"Feature\": X_train.columns\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(grid_fe)\n",
    "\n",
    "# ------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ------------------------------\n",
    "\n",
    "perm_imp_grid = permutation_importance(\n",
    "    sales_lgb,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "perm_imp_grid_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': perm_imp_grid.importances_mean,\n",
    "    'Std': perm_imp_grid.importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(\n",
    "    perm_imp_grid_df['Feature'],\n",
    "    perm_imp_grid_df['Importance'],\n",
    "    xerr=perm_imp_grid_df['Std']\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Mean Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6afbf-5361-4a63-abb2-e8feb31805be",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "threshold = 0.5\n",
    "\n",
    "val_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train_fold = X.iloc[train_idx].copy()\n",
    "    X_val_fold   = X.iloc[val_idx].copy()\n",
    "    y_train_fold = y.iloc[train_idx].copy()\n",
    "    y_val_fold   = y.iloc[val_idx].copy()\n",
    "\n",
    "    sales_lgb.fit(X_train_fold, y_train_fold, categorical_feature=cat_cols)\n",
    "\n",
    "    y_train_proba = sales_lgb.predict_proba(X_train_fold)[:, 1]\n",
    "    y_val_proba   = sales_lgb.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "    y_val_pred   = (y_val_proba >= threshold).astype(int)\n",
    "\n",
    "    train_score = balanced_accuracy_score(y_train_fold, y_train_pred)\n",
    "    val_score   = balanced_accuracy_score(y_val_fold, y_val_pred)\n",
    "\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"    Training Balanced Accuracy:   {train_score:.4f}\")\n",
    "    print(f\"    Validation Balanced Accuracy: {val_score:.4f}\\n\")\n",
    "\n",
    "# Show mean CV score\n",
    "print(f\"Mean Validation Balanced Accuracy: {np.mean(val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d658c22-1431-44b4-97d8-c058ee47fe60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47509f63-9abb-4c6e-9ba0-09f8e79350fa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute balanced sample weights\n",
    "weights = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "\n",
    "sales_xg = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=1234,\n",
    "    tree_method=\"hist\",\n",
    "    enable_categorical=True,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [200, 400, 600, 800],\n",
    "    \"learning_rate\": np.linspace(0.01, 0.2, 10),\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"min_child_weight\": [1, 2, 3, 5],\n",
    "    \"subsample\": [0.5, 0.7, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 0.85, 1.0],\n",
    "    \"colsample_bylevel\": [0.5, 0.7, 1.0],\n",
    "    \"colsample_bynode\": [0.5, 0.7, 0.85, 1.0],\n",
    "    \"gamma\": [0, 0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "    \"reg_alpha\": [0, 0.1, 0.5, 1.0],\n",
    "    \"reg_lambda\": [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=sales_xg,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=1234\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train, **{\"sample_weight\": weights})\n",
    "\n",
    "print(\"Best params:\", random_search.best_params_)\n",
    "print(\"Best CV balanced accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7f8f8-83fc-489a-8d83-36c72b574e44",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "sales_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.1788,\n",
    "    max_depth=5,\n",
    "    min_child_weight=2,\n",
    "    subsample=1,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel = 1,\n",
    "    colsample_bynode = 0.5,\n",
    "    gamma=0.3,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=1,\n",
    "\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\", \n",
    "    random_state=1234,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    enable_categorical = True\n",
    ")\n",
    "\n",
    "\n",
    "sales_xgb.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False,\n",
    "    sample_weight = weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85907743-f456-4fed-9596-89ac545fa184",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#### XGBoost ####\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#### Validating the XGB Model ####\n",
    "\n",
    "y_train_proba = sales_xgb.predict_proba(X_train)[:, 1]\n",
    "y_test_proba  = sales_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_test_pred  = (y_test_proba  >= threshold).astype(int)\n",
    "\n",
    "print(\"Træningsdata:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Testdata:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Importance (CatBoost)\n",
    "# ------------------------------\n",
    "\n",
    "model_featureimportance = sales_xgb.feature_importances_\n",
    "\n",
    "grid_fe = pd.DataFrame({\n",
    "    \"Importance\": model_featureimportance,\n",
    "    \"Feature\": X_train.columns\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(grid_fe)\n",
    "\n",
    "# ------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ------------------------------\n",
    "\n",
    "perm_imp_grid = permutation_importance(\n",
    "    sales_xgb,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "perm_imp_grid_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': perm_imp_grid.importances_mean,\n",
    "    'Std': perm_imp_grid.importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(\n",
    "    perm_imp_grid_df['Feature'],\n",
    "    perm_imp_grid_df['Importance'],\n",
    "    xerr=perm_imp_grid_df['Std']\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Mean Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f453991-5374-4d90-b7f0-be918309cda1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)\n",
    "\n",
    "thresholds = np.arange(0.1, 0.91, 0.05)\n",
    "threshold_scores = {t: [] for t in thresholds}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    sales_xgb.fit(X_train, y_train,\n",
    ")\n",
    "\n",
    "    y_val_proba = sales_xgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_val_pred = (y_val_proba >= t).astype(int)\n",
    "        score = balanced_accuracy_score(y_val, y_val_pred)\n",
    "        threshold_scores[t].append(score)\n",
    "\n",
    "    print(f\"Fold {fold} complete.\")\n",
    "\n",
    "mean_scores = {t: np.mean(scores) for t, scores in threshold_scores.items()}\n",
    "best_threshold = max(mean_scores, key=mean_scores.get)\n",
    "best_score = mean_scores[best_threshold]\n",
    "\n",
    "print(\"\\n Threshold Tuning Results:\")\n",
    "for t in sorted(mean_scores):\n",
    "    print(f\"Threshold {t:.2f}: Mean Balanced Accuracy = {mean_scores[t]:.4f}\")\n",
    "\n",
    "print(f\"\\n Best Threshold: {best_threshold:.2f} with Mean Balanced Accuracy = {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623980fe-ecee-4d86-9dbc-54ef5c5fc059",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "threshold = 0.5\n",
    "\n",
    "val_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train_fold = X.iloc[train_idx].copy()\n",
    "    X_val_fold   = X.iloc[val_idx].copy()\n",
    "    y_train_fold = y.iloc[train_idx].copy()\n",
    "    y_val_fold   = y.iloc[val_idx].copy()\n",
    "\n",
    "    weights_fold = compute_sample_weight(class_weight=\"balanced\", y=y_train_fold)\n",
    "\n",
    "    sales_xgb.fit(X_train_fold, y_train_fold, sample_weight=weights_fold)\n",
    "\n",
    "    y_train_proba = sales_xgb.predict_proba(X_train_fold)[:, 1]\n",
    "    y_val_proba   = sales_xgb.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "    y_val_pred   = (y_val_proba >= threshold).astype(int)\n",
    "\n",
    "    train_score = balanced_accuracy_score(y_train_fold, y_train_pred)\n",
    "    val_score   = balanced_accuracy_score(y_val_fold, y_val_pred)\n",
    "\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"    Training Balanced Accuracy:   {train_score:.4f}\")\n",
    "    print(f\"    Validation Balanced Accuracy: {val_score:.4f}\\n\")\n",
    "\n",
    "# Show mean CV score\n",
    "print(f\"Mean Validation Balanced Accuracy: {np.mean(val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e5912-c946-41fb-87e2-4f7514f5f812",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_xgb_importance(model, importance_type=\"gain\", top_n=20):\n",
    "    imp = model.get_booster().get_score(importance_type=importance_type)\n",
    "\n",
    "    df_imp = (\n",
    "        pd.DataFrame({\"Feature\": list(imp.keys()), \"Importance\": list(imp.values())})\n",
    "        .sort_values(\"Importance\", ascending=False)\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(\n",
    "        data=df_imp.head(top_n),\n",
    "        x=\"Importance\",\n",
    "        y=\"Feature\",\n",
    "        palette= sns.color_palette(\"tab20\")\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Variablernes vigtighed til modellen ({importance_type})\", fontsize=14, weight=\"bold\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_xgb_importance(sales_xgb, top_n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53af915-c736-4f09-a07f-9f2be4f58f7c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def plot_xgb_with_permutation(model, X_test, y_test, importance_type=\"gain\", top_n=20):\n",
    "\n",
    "    imp = model.get_booster().get_score(importance_type=importance_type)\n",
    "\n",
    "    df_gain = (\n",
    "        pd.DataFrame({\"Feature\": list(imp.keys()), \"Gain\": list(imp.values())})\n",
    "        .sort_values(\"Gain\", ascending=False)\n",
    "    )\n",
    "\n",
    "    # Tag top-N fra gain importance\n",
    "    top_features = df_gain.head(top_n)[\"Feature\"].tolist()\n",
    "\n",
    "\n",
    "    perm = permutation_importance(\n",
    "        model, X_test, y_test,\n",
    "        n_repeats=10,\n",
    "        random_state=123,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    df_perm = pd.DataFrame({\n",
    "        \"Feature\": X_test.columns,\n",
    "        \"Permutation\": perm.importances_mean\n",
    "    })\n",
    "\n",
    "    # Filtrer permutation importance til kun gain-top features\n",
    "    df_perm = df_perm[df_perm[\"Feature\"].isin(top_features)]\n",
    "\n",
    "    df_perm = df_perm.set_index(\"Feature\").loc[top_features].reset_index()\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8), sharey=True)\n",
    "    fig.suptitle(\"DaysBetweenCreateClose er den klart mest betydelige variabel i modellen\", fontsize=18, weight=\"bold\", y=1.02)\n",
    "\n",
    "    # Gain plot\n",
    "    sns.barplot(\n",
    "        data=df_gain.head(top_n),\n",
    "        x=\"Gain\", y=\"Feature\",\n",
    "        palette=sns.color_palette(\"tab20\"),\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f\"XGBoost variabel vigtighed (gain)\", fontsize=14, weight=\"bold\")\n",
    "    axes[0].set_xlabel(\"Bidrag til modelpræcision (gain)\")\n",
    "    axes[0].set_ylabel(\"Variabel\")\n",
    "\n",
    "    # Permutation plot\n",
    "    sns.barplot(\n",
    "        data=df_perm,\n",
    "        x=\"Permutation\", y=\"Feature\",\n",
    "        palette=sns.color_palette(\"tab20\"),\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(\"Permutation feature importance (XGBoost model)\", fontsize=14, weight=\"bold\")\n",
    "    axes[1].set_xlabel(\"gennemsnitlig reduktion i modelperformance ved permutation\")\n",
    "    axes[1].set_ylabel(\"\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_xgb_with_permutation(sales_xgb, X_test, y_test, top_n=20)\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "43b504ef-d5b9-4d75-9d65-bfc320a8736c",
    "default_lakehouse_name": "ml_curate_lakehouse",
    "default_lakehouse_workspace_id": "19522171-0292-4a1f-854f-4941b77c2765",
    "known_lakehouses": [
     {
      "id": "43b504ef-d5b9-4d75-9d65-bfc320a8736c"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
